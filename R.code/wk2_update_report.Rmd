---
title: "Capstone Project Update Week 2"
subtitle: "Exploratory Analysis and Strategy for Prediction Modeling"
author: "Joe Rubash"
date: "December 8th, 2018"
output: html_document
editor_options: 
  chunk_output_type: inline
---

### Introduction: 

```{r setup - loading and formating data, echo=FALSE, include=FALSE, warning=FALSE, message=FALSE}
# global settings for chunks
knitr::opts_chunk$set(echo=FALSE)

# clear environment
rm(list = ls())

# required libraries
source("./Loading_cleaning_data.R")
```                              


## Exploration of data:

```{r ---Unigram tokens all data---, echo=FALSE, include=FALSE, warning=FALSE, message=FALSE}
# Add line variable to data----
all.data.blogs <- tibble(line = 1:length(data.raw.blogs),
                     txt_sample = data.raw.blogs)
all.data.news <- tibble(line = 1:length(data.raw.news),
                    txt_sample = data.raw.news)
all.data.twitter <- tibble(line = 1:length(data.raw.twitter),
                       txt_sample = data.raw.twitter)

# Tokenize to individual words
unigram_tokens <- function(x, y){
    df <- unnest_tokens(tbl = x,
                        output = words,
                        input = txt_sample,
                        token = "words",
                        to_lower = TRUE)
    df.names <- paste("all.data.unigram", y, sep = ".")
    assign(df.names, df, envir = .GlobalEnv)
    }

raw.data.list <- list(all.data.blogs,
                      all.data.news,
                      all.data.twitter)
sources <- c("blogs",
             "news",
             "twitter")

map2(.x = raw.data.list, .y = sources, .f = unigram_tokens)
```

### Data summary statistics:
```{r data summary stats, warning=FALSE, message=FALSE}
raw.data.list <- list(data.raw.blogs,
                      data.raw.news,
                      data.raw.twitter)
sources <- c("blogs",
             "news",
             "twitter")
all.data.unigram <- list(all.data.unigram.blogs,
                         all.data.unigram.news,
                         all.data.unigram.twitter)

raw_data_summary_stats <- function(x, y, z){
    num.lines <- length(x)
    data.size <- object.size(x) %>% 
        as.character() %>% 
        as.numeric()/1000000
    unique.words <- z %>% 
        count(words, sort = TRUE) %>% 
        nrow()
    data.source <- y
    df <- data.frame(data.source,
                     data.size,
                     num.lines,
                     unique.words)
}

pmap_df(.l = list(raw.data.list,
                  sources,
                  all.data.unigram),
        .f = raw_data_summary_stats) %>% 
    kable()
```
```{r ---Unigram tokens---, include=FALSE}
# Tokenize to individual words
unigram_tokens <- function(x, y){
    df <- unnest_tokens(tbl = x,
                        output = words,
                        input = txt_sample,
                        token = "words",
                        to_lower = TRUE)
    df.names <- paste("data.unigram", y, sep = ".")
    assign(df.names, df, envir = .GlobalEnv)
    }

map2(.x = data.list, .y = sources, .f = unigram_tokens)
```

### Distributions of words occurences for each data set 
```{r dist of word occurances log scale, out.width = '100%' }
data.unigram.list <- list(data.unigram.blogs, data.unigram.news, data.unigram.twitter)
sources <- c("blogs", "news", "twitter")

most_common_words <- function(x, y){
    x %>%
        count(words, sort = TRUE) %>%
        mutate(source = y)
    }
        
map2_df(.x = data.unigram.list, .y = sources, .f = most_common_words)%>% 
    ggplot(aes(x = n, fill = source))+
    facet_grid(source ~ .)+
    geom_histogram(bins = 100)+
    #geom_rug()+
    scale_x_log10()+
    scale_y_log10()

```

### How many unique words are needed to represent 50 and 90% of the data?
```{r cummulative most freq words, out.width = '100%'}
# determine the relationship of number of unique words to percent of data set they
# represent
data.token.1.list <- list(data.unigram.blogs, data.unigram.news, data.unigram.twitter)
sources <- c("blogs", "news", "twitter")


perc_unique_words_cover <- function(x, y){
    x %>% 
    count(words, sort = TRUE) %>%
    mutate(cum.sum = cumsum(n),
           perc.total = cum.sum/nrow(x),
           top.word.n = 1:nrow(.),
           perc.unique.words = top.word.n/nrow(.),
           source = y)
    }

map2_df(.x = data.token.1.list,
        .y = sources,
        .f = perc_unique_words_cover) %>% 
    ggplot(aes(x = perc.unique.words, 
               y = perc.total,
               color = source))+
    geom_point()+
    geom_hline(yintercept = c(0.5, 0.9), linetype = 2)+
    scale_y_continuous(labels = percent)+
    scale_x_continuous(labels = percent)+
    facet_grid(. ~ source)
```

```{r ---Bigram tokens---, include=FALSE}
# determine top 10 words
top.first.words.blogs <- most_common_words(data.unigram.blogs, "blogs") %>% 
    top_n(n, n=10)

# remove unigram data sets to save memory
rm(data.unigram.blogs, data.unigram.news, data.unigram.twitter)

# Tokenize to two word groups
bigram_tokens <- function(x, y){
    df <- unnest_tokens(tbl = x,
                        output = words,
                        input = txt_sample,
                        token = "ngrams",
                        n = 2,
                        to_lower = TRUE)
    df.names <- paste("data.bigram", y, sep = ".")
    assign(df.names, df, envir = .GlobalEnv)
    }

map2(.x = data.list, .y = sources, .f = bigram_tokens)
```


### Most common second word given previous word
```{r two word frequency dist in blogs, out.width = '100%'}

most.freq.2nd.word <- data.bigram.blogs %>% 
    separate(words, " ", into = c("first.word", "second.word")) %>% 
    group_by(first.word) %>% 
    count(second.word, sort = TRUE) %>% 
    top_n(n, n = 5) %>%
    mutate(temp = paste(second.word, first.word, sep = "_")) %>%
    filter(first.word %in% top.first.words.blogs$words) %>% 
    ungroup() %>% 
    arrange(first.word, n) %>% 
    mutate(.r = row_number())

#windows()
    ggplot(data = most.freq.2nd.word,
           aes(x = .r,
               y = n,
               fill = first.word))+
    geom_col(position = position_dodge())+
    scale_x_continuous(  # This handles replacement of .r for x
    breaks = most.freq.2nd.word$.r,     # notice need to reuse data frame
    labels = most.freq.2nd.word$second.word) +
    coord_flip()+
    facet_wrap(~ first.word, scales = "free")

```

### Plan to model word data to predict next word:  
- Sample a portion of data   
- Tokenize to bigram and perhaps trigram   
- Convert to document term matrix   
- Use Randomforest to generate predictive model   
### Plan for shiny app:   
- include input field for user to type one word string   
- button to run prediction   
- output will be a word cloud with most likely word as largest with other options as smaller surrounding  