---
title: "Model validation and testing"
author: "Joe Rubash"
date: "January 8th, 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---


```{r setup}
# clear environment
rm(list = ls())

knitr::opts_chunk$set(echo = TRUE)

# required libraries
library(tidyverse)
library(readr)
library(readtext)
library(purrr)
library(quanteda)
library(tm)
library(data.table)
library(stringi)
library(stringr)
library(knitr)
library(rbenchmark)
library(doParallel)
library(R.utils)

# Functions ####################################################################

# create function to read in convert, sample then write to file text data
sample_data <- function(file){
    readLines(con = file,
              skipNul = TRUE,
              encoding="UTF-8") %>%
    sample(.,
           size = sample.size * length(.),
           replace = FALSE)
    }

# function to create dfm matrices of different ngrams
dfm_ngrams <- function(corpus, ngrams){
    dfm(corpus, what = "word",
        tolower = TRUE,
        remove_numbers = TRUE,
        remove_punct = TRUE,
        remove_symbols = TRUE,
        remove_separators = TRUE,
        remove_twitter = TRUE, 
        remove_hyphens = TRUE,
        remove_url = TRUE,
        ngrams = ngrams,
        skip = 0L,
        concatenator = "_",
        verbose = quanteda_options("verbose")) %>% 
        colSums(.)
}

# function to take dfm convert to data.table then split into base and pred and 
# calculate probabilities
split_base_pred_freq <- function(dfm.ngram){
    #dfm.ngram <- bigrams.test
    dt <- data.table(feats = names(dfm.ngram), count = as.data.table(dfm.ngram))
    dt <- dt[, feats := stri_replace_last_fixed(feats,"_", " ")]
    dt <- dt[, c("base", "pred") 
      := data.table(stri_split_fixed(feats, " ", simplify = TRUE))]
    dt <- dt[, feats := NULL]
    dt <- setkey(dt, base)
    dt <- dt[, `:=` (base.count = sum(count.dfm.ngram)),
             key = base]
    dt <- dt[count.dfm.ngram > 1]
    dt <- dt[, prob := count.dfm.ngram / base.count]
    dt <- dt[ , .SD[which.max(count.dfm.ngram)],
              key = base]
    dt <- dt[, count.dfm.ngram := NULL]
    dt <- dt[, base.count := NULL]
    dt <- dt[str_which(base, "[[:ascii:]]+")] # extract only ascii characters
    dt <- dt[-(str_which(base, "[:digit:]"))] # remove words with numbers
    return(dt)
}

# function to run validation data through model
predict_next_word <- function(prev.words, model = ng.model){
    string <- str_replace_all(prev.words, "_", " ")
    n = 4:1
    patt = sprintf("\\w+( \\w+){0,%d}$", n-1)
    test.input <- data.table(base = stri_extract(string, regex = patt))
    test.input <- test.input[, order := length(test.input$base):1]
    prep.input <- test.input[, base := str_replace_all(test.input$base, "\\s+", "_") %>% 
        tolower()] 
    prep.input <- setkey(prep.input, base)
    prep.input <- prep.input$base
    results.dt <- model[base %in% prep.input]
    result <- results.dt[order(-prob)[1], .(base, pred)]
    print(result)
    }


# Sample data ##################################################################
# download and unzip data folder if needed
if(!dir.exists("../data/Coursera-SwiftKey/")){
    file.url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
    temp <- tempfile()
    download.file(file.url, temp, mode = "wb")
    unzip(temp, exdir = "../data/Coursera-SwiftKey")
    rm(temp)
}

# read in all text data
blogs <- readLines(con = "../data/Coursera-SwiftKey/final/en_US/en_US.blogs.txt",
                   skipNul = TRUE,
                   encoding="UTF-8")
news <- readLines(con = file("../data/Coursera-SwiftKey/final/en_US/en_US.news.txt", "rb"),
                  skipNul = TRUE,
                  encoding="UTF-8")
twitter <-  readLines(con = "../data/Coursera-SwiftKey/final/en_US/en_US.twitter.txt",
                      skipNul = TRUE,
                      encoding="UTF-8")

# Combine all text data
all.text <- c(blogs, news, twitter) 

# remove unneeded objects
rm(blogs, news, twitter)

valid.size <- 0.05
test.size <- 0.05

# grab randomly selected validation and test data from all.text
set.seed(123)
valid <- sample(all.text,
                   size = valid.size * length(all.text),
                   replace = FALSE)
test <- sample(all.text[-which(valid %in% all.text)],
                   size = test.size * length(all.text),
                   replace = FALSE) 

# remove unneeded objects
rm(all.text)

```
```{r Read in model}
# read in previously built model from file
#ng.model <- readRDS(file = "./NExt_word_prediction/model.data/ngrams.mod.3.rds")

#ng.model <- readRDS(file = "./NExt_word_prediction/model.data/ngrams.freq.15.rds")

#ng.model <- readRDS(file = "./NExt_word_prediction/model.data/ngrams.freq.25.rds")

#ng.model <- readRDS(file = "./NExt_word_prediction/model.data/ngrams.mod.3.rds")

ng.model <- readRDS(file = "./NExt_word_prediction/model.data/ngrams.mod.4.clean.rds")


```
```{r prep validation data}
# Prepare corpus by sentences
corpus.sentences <- corpus(valid) %>%
    corpus_reshape(., to = "sentence") 

# Create hexagrams for validation
hexagram <- dfm_ngrams(corpus = corpus.sentences, ngrams = 6L)

# split hexagram into base and prediction
valid.hexagram <- split_base_pred_freq(dfm.ngram = hexagram)
rm(hexagram, corpus.sentences)

# further prepare validation data for entry into prediction algorithm
valid.hexagram.base <- valid.hexagram[["base"]]
valid.hexagram.pred <- valid.hexagram[["pred"]]

saveRDS(valid.hexagram.base, file = "./R.code/shiny.presentation.data/valid.hexagram.base.rds")
saveRDS(valid.hexagram.pred, file = "./R.code/shiny.presentation.data/valid.hexagram.pred.rds")

```
```{r Evaluate model}
ptm <- proc.time() # start time
predict_next_word(prev.words = valid.hexagram.base[58])
proc.time() - ptm # end time and difference


# validation of models 

# randomly sample from validation data set
valid.size <- 2000
seed <- 562
set.seed(seed)
valid.base.sample <- sample(valid.hexagram.base, valid.size, replace = FALSE)

# associated actual prediction for sample of validation data
set.seed(seed)
valid.pred.sample <- sample(valid.hexagram.pred, valid.size, replace = FALSE)

ptm <- proc.time() # start time
model.test <- map_df(.x = valid.base.sample, .f = predict_next_word) %>%
    mutate(actual.pred = valid.pred.sample,
           result = pred == actual.pred)
succes.rate <- model.test %>% summarise(correct.pred.rate = sum(result[!is.na(result)])/length(result),
                         no.pred = sum(is.na(result))/length(result))
paste("Model correctly predicted next word", succes.rate[[1]]*100, "% of the time")

time.per.test <- (proc.time() - ptm)[3] / valid.size # end time and difference

paste("The model and test function took rouhgly", round(time.per.test[[1]], 5), "seconds per test")

```
1/10/2019 1:00 pm
ngrams.model.3.rds
10,000 hexagram tests from validation data
 correct.pred.rate no.pred
1            0.5923   0.003
 elapsed 
0.005929 sec per test

1/10/2019 2:06 pm
ngrams.model.3.rds
500 hexagram tests from validation data
 correct.pred.rate no.pred
1             0.618   0.004
elapsed 
0.00528 sec per test

1/10/2019 2:03 pm
ngrams.freq.15.rds
500 hexagram tests from validation data
correct.pred.rate no.pred
1             0.328       0
elapsed 
 3.6179 sec per test
 
1/10/2019 2:17 pm
ngrams.freq.25.rds
500 hexagram tests from validation data
 correct.pred.rate no.pred
1             0.216   0.002
elapsed 
0.84734 sec per test

1/10/2019 4:43 pm
ngrams.model.4.rds
500 hexagram tests from validation data
correct.pred.rate no.pred
1             0.614   0.004
elapsed 
0.00578  sec per test

1/11/2019 8:49 pm
ngrams.model.4.rds after removing rows with profanity in prediction words
2000 hexagram tests from validation data
correct.pred.rate no.pred
1            0.5965  0.0045
elapsed 
0.005765 sec per test

```{r week 3 quiz}
# 1. The guy in front of me just bought a pound of bacon, a bouquet, and a case of
# soda
# beer - answer good
# cheese
# pretzels

# 2. You're the reason why I smile everyday. Can you follow me please? It would mean the
# universe
# most
# best
# world - answer good

# 3. Hey sunshine, can you follow me and make me the
# saddest
# smelliest
# happiest - answer no
# bluest

# 4. Very early observations on the Bills game: Offense still struggling but the
# crowd
# players
# referees
# defense - answer no

# 5. Go on a romantic date at the
# mall
# beach - answer no
# grocery
# movies

# 6. Well I'm pretty sure my granny has some old bagpipes in her garage I'll dust them off and be on my
# phone
# way - answer close (would work if I dropped count under 2)
# horse
# motorcycle

# 7. Ohhhhh #PointBreak is on tomorrow. Love that film and haven't seen it in quite some
# weeks
# thing
# time - answer yes (better if I drop count of 1)
# years

# 8. After the ice bucket challenge Louis will push his long wet hair out of his eyes with his little
# toes
# eyes
# ears
# fingers - answer no

# 9. Be grateful for the good times and keep the faith during the
# sad
# worse
# bad - answer no
# hard

# 10. If this isn't the cutest thing you've ever seen, then you must be
# callous
# insensitive
# asleep
# insane - answer no

```


