---
title: "Getting_n_cleaning_data"
author: "Joe Rubash"
date: "November 28, 2018"
output: html_document
---

# Todo:
- ensure that file has been read in correctly
- where are the tags for the corporas that was mentioned?
- figureout how to remove bad language
- consider changing sample data back to text file then read text... may be faster

# ##############################################################################
```{r setup, include=FALSE}
# clear environment
rm(list = ls())


# Libraries----
library(tidyverse)
library(readr)
library(tidytext)
library(stringr)
library(purrr)
library(quanteda)
library(tm)


# Themes----


# Fucntions-----
cleanFiles<-function(file,newfile){
    writeLines(iconv(readLines(file,skipNul = TRUE)),newfile)
}

# Load data----
# download and unzip data folder
if(!dir.exists("../data/Coursera-SwiftKey/")){
    file.url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
    temp <- tempfile()
    download.file(file.url, temp, mode = "wb")
    unzip(temp, exdir = "../data/Coursera-SwiftKey")
    rm(temp)
}


# Load then sample a portion of data then save to file
sample.size <- 0.25 # proportion of orignal text data to save to file

# load en_US.blogs.txt----
if(file.exists("../data/temp.data/data.blogs.txt")){
    sample.blogs <- read_tsv("../data/temp.data/data.blogs.txt")
} else {
    data.raw.blogs <- readLines("../data/Coursera-SwiftKey/final/en_US/en_US.blogs.txt",
                                    skipNul = TRUE,
                                    encoding="UTF-8")
    # create a sample
    sample.blogs <- sample(data.raw.blogs,
                           size = sample.size * length(data.raw.blogs),
                           replace = FALSE)
    # write the sample to txt file in "output"
    writeLines(sample.blogs,
            "../data/temp.data/sample.blogs.txt")
    # remove original data
    rm(data.raw.blogs)
    }

# load en_US.news.txt----
if(file.exists("../data/temp.data/data.news.txt")){
    sample.news <- read_tsv("../data/temp.data/data.news.txt")
} else {
    data.raw.news <- readLines(con = file("../data/Coursera-SwiftKey/final/en_US/en_US.news.txt", "rb"),
                                    skipNul = TRUE)
    # create a sample
    sample.news <- sample(data.raw.news,
                           size = sample.size * length(data.raw.news),
                           replace = FALSE)
    # write the sample to txt file in "output"
    writeLines(sample.news,
            "../data/temp.data/sample.news.txt")
    # remove original data
    rm(data.raw.news)
    }

# load en_US.twitter.txt----
if(file.exists("../data/temp.data/data.twitter.txt")){
    sample.twitter <- read_tsv("../data/temp.data/data.twitter.txt")
} else {
    data.raw.twitter <- readLines("../data/Coursera-SwiftKey/final/en_US/en_US.twitter.txt",
                                    skipNul = TRUE,
                                    encoding="UTF-8")
    # create a sample
    sample.twitter <- sample(data.raw.twitter,
                           size = sample.size * length(data.raw.twitter),
                           replace = FALSE)
    # write the sample to txt file in "output"
    writeLines(sample.twitter,
            "../data/temp.data/sample.twitter.txt")
    # remove original data
    rm(data.raw.twitter)
}

# Add line variable to data----
data.blogs <- tibble(line = 1:length(sample.blogs),
                         txt_sample = sample.blogs)
data.news <- tibble(line = 1:length(sample.news),
                         txt_sample = sample.news)
data.twitter <- tibble(line = 1:length(sample.twitter),
                         txt_sample = sample.twitter)

# Tokenize data to one word per row----
data.blogs <- unnest_tokens(tbl = data.blogs,
                            output = words,
                            input = txt_sample,
                            token = "words",
                            to_lower = FALSE)
data.news <- unnest_tokens(tbl = data.news,
                            output = words,
                            input = txt_sample,
                            token = "words",
                            to_lower = FALSE)
data.twitter <- unnest_tokens(tbl = data.twitter,
                            output = words,
                            input = txt_sample,
                            token = "words",
                            to_lower = FALSE)

# Tokenize data to two words per row----
#data.blogs <- unnest_tokens(tbl = data.blogs,
#                            output = words,
#                            input = txt_sample,
#                            token = "words",
#                            to_lower = FALSE)
#data.news <- unnest_tokens(tbl = data.news,
#                            output = words,
#                            input = txt_sample,
#                            token = "words",
#                            to_lower = FALSE)
#data.twitter <- unnest_tokens(tbl = data.twitter,
#                            output = words,
#                            input = txt_sample,
#                            token = "words",
#                            to_lower = FALSE)

# Tokenize data to three words per row----
#data.blogs <- unnest_tokens(tbl = data.blogs,
#                            output = words,
#                            input = txt_sample,
#                            token = "words",
#                            to_lower = FALSE)
#data.news <- unnest_tokens(tbl = data.news,
#                            output = words,
#                            input = txt_sample,
#                            token = "words",
#                            to_lower = FALSE)
#data.twitter <- unnest_tokens(tbl = data.twitter,
#                            output = words,
#                            input = txt_sample,
#                            token = "words",
#                            to_lower = FALSE)
```                            
# ##############################################################################

# Quiz week 1
```{r longest lines}
# longest lines in data sets
map_dbl(data.raw.blogs, nchar) %>% 
    max()

map_dbl(data.raw.news, nchar) %>% 
    max()

map_dbl(data.raw.twitter, nchar) %>% 
    max()

```
```{r love vs hate}
lines_love <- data.twitter %>% 
    mutate(love = str_detect(words, "love"))%>%
    group_by(line) %>%
    summarise(love_by_line = sum(love)) %>% 
    filter(love_by_line != 0)

lines_hate <- data.twitter %>% 
    mutate(hate = str_detect(words, "hate"))%>%
    group_by(line) %>%
    summarise(hate_by_line = sum(hate)) %>% 
    filter(hate_by_line != 0)


dim(lines_love)[1] / dim(lines_hate)[1]
```
```{r biostats tweet}
data.raw.twitter[str_which(data.raw.twitter, " biostats ")]
```

```{r number tweets matching phrase}
str_which(data.raw.twitter, "A computer once beat me at chess, but it was no match for me at kickboxing") %>% 
    length()
```
#.
# Exploration
## Expectations in data:
- expecting words to be spelled correctly... most likely not true
- expecting tokenizer to correctly split words and drop punctuation... mostlikely not true
- 
```{r distribution of line lengths in blogs}
nwords.data.blogs <- data.blogs%>%
    group_by(line) %>% 
    summarise(nwords = n()) %>% 
    mutate(source = "blogs")

nwords.data.news <- data.news%>%
    group_by(line) %>% 
    summarise(nwords = n()) %>% 
    mutate(source = "news")

nwords.data.twitter <- data.twitter%>%
    group_by(line) %>% 
    summarise(nwords = n()) %>% 
    mutate(source = "twitter")

nwords.data <- rbind(nwords.data.blogs,
                     nwords.data.news,
                     nwords.data.twitter)

ggplot(nwords.data, aes(x = nwords, fill = source))+
    facet_grid(source ~ .)+
    geom_histogram(bins = 100)+
    geom_rug()+
    scale_x_log10()
    
```
```{r distribution of line lengths in news}
data.news %>% 
    group_by(line) %>% 
    summarise(nwords = n()) %>% 
    ggplot(aes(x = nwords))+
    geom_histogram(bins = 100)+
    scale_x_log10()
```
```{r most and common words}
most.common.blog <- data.blogs %>%
    count(words, sort = TRUE) %>% 
    head(n = 100) %>% 
    select(words) %>% 
    rename(blog.words = words)
   
most.common.news <- data.news %>%
    count(words, sort = TRUE) %>% 
    head(n = 100)%>% 
    select(words)%>% 
    rename(news.words = words)

most.common.twitter <- data.twitter %>%
    count(words, sort = TRUE) %>% 
    head(n = 100)%>% 
    select(words)%>% 
    rename(twitter.words = words)

most.common.words <- bind_cols(most.common.blog, most.common.news, most.common.twitter)

```
```{r bad words}
unique(data.blogs$words)[unique(data.blogs$words) %in% c("fuck", "shit", "pussy")]
```
