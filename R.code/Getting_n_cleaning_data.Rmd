---
title: "Getting_n_cleaning_data"
author: "Joe Rubash"
date: "November 28, 2018"
output: html_document
---

# Todo:
- ensure that file has been read in correctly
- where are the tags for the corporas that was mentioned?
- figureout how to remove bad language
- consider changing sample data back to text file then read text... may be faster

# ##############################################################################
```{r setup, include=FALSE}
# clear environment
rm(list = ls())


# Libraries----
library(tidyverse)
library(readr)
library(tidytext)
library(stringr)
library(purrr)
library(quanteda)
library(tm)
library(hunspell)
library(pacman)
p_load("cld2")


# Themes----


# Fucntions-----


# Load data----
# download and unzip data folder
if(!dir.exists("../data/Coursera-SwiftKey/")){
    file.url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
    temp <- tempfile()
    download.file(file.url, temp, mode = "wb")
    unzip(temp, exdir = "../data/Coursera-SwiftKey")
    rm(temp)
}


# Load then sample a portion of data then save to file
sample.size <- 0.1 # proportion of orignal text data to save to file

# load en_US.blogs.txt----
if(file.exists("../data/temp.data/data.blogs.txt")){
    sample.blogs <- read_tsv("../data/temp.data/data.blogs.txt")
} else {
    data.raw.blogs <- readLines("../data/Coursera-SwiftKey/final/en_US/en_US.blogs.txt",
                                    skipNul = TRUE,
                                    encoding="UTF-8")
    # create a sample
    sample.blogs <- sample(data.raw.blogs,
                           size = sample.size * length(data.raw.blogs),
                           replace = FALSE)
    # write the sample to txt file in "output"
    writeLines(sample.blogs,
            "../data/temp.data/sample.blogs.txt")
    # remove original data
    rm(data.raw.blogs)
    }

# load en_US.news.txt----
if(file.exists("../data/temp.data/data.news.txt")){
    sample.news <- read_tsv("../data/temp.data/data.news.txt")
} else {
    data.raw.news <- readLines(con = file("../data/Coursera-SwiftKey/final/en_US/en_US.news.txt", "rb"),
                                    skipNul = TRUE)
    # create a sample
    sample.news <- sample(data.raw.news,
                           size = sample.size * length(data.raw.news),
                           replace = FALSE)
    # write the sample to txt file in "output"
    writeLines(sample.news,
            "../data/temp.data/sample.news.txt")
    # remove original data
    rm(data.raw.news)
    }

# load en_US.twitter.txt----
if(file.exists("../data/temp.data/data.twitter.txt")){
    sample.twitter <- read_tsv("../data/temp.data/data.twitter.txt")
} else {
    data.raw.twitter <- readLines("../data/Coursera-SwiftKey/final/en_US/en_US.twitter.txt",
                                    skipNul = TRUE,
                                    encoding="UTF-8")
    # create a sample
    sample.twitter <- sample(data.raw.twitter,
                           size = sample.size * length(data.raw.twitter),
                           replace = FALSE)
    # write the sample to txt file in "output"
    writeLines(sample.twitter,
            "../data/temp.data/sample.twitter.txt")
    # remove original data
    rm(data.raw.twitter)
}

# Add line variable to data----
data.blogs <- tibble(line = 1:length(sample.blogs),
                         txt_sample = sample.blogs)
data.news <- tibble(line = 1:length(sample.news),
                         txt_sample = sample.news)
data.twitter <- tibble(line = 1:length(sample.twitter),
                         txt_sample = sample.twitter)


# Tokenize data ----
# list of data frames to manipulate
data.list <- list(data.blogs, data.news, data.twitter)

# list of names for tokenized to words data frames
sources <- c("blogs", "news", "twitter")

# Tokenize to individual words
token_1_words <- function(x, y){
    df <- unnest_tokens(tbl = x,
                        output = words,
                        input = txt_sample,
                        token = "words",
                        to_lower = FALSE)
    df.names <- paste("data.one.words", y, sep = ".")
    assign(df.names, df, envir = .GlobalEnv)
    }

map2(.x = data.list, .y = sources, .f = token_1_words)


# Tokenize to two word groups
token_2_words <- function(x, y){
    df <- unnest_tokens(tbl = x,
                        output = words,
                        input = txt_sample,
                        token = "ngrams",
                        n = 2,
                        to_lower = FALSE)
    df.names <- paste("data.two.words", y, sep = ".")
    assign(df.names, df, envir = .GlobalEnv)
    }

map2(.x = data.list, .y = sources, .f = token_2_words)
 
```                            
# ##############################################################################

# Quiz week 1
```{r longest lines}
# longest lines in data sets
map_dbl(data.raw.blogs, nchar) %>% 
    max()

map_dbl(data.raw.news, nchar) %>% 
    max()

map_dbl(data.raw.twitter, nchar) %>% 
    max()

```
```{r love vs hate}
lines_love <- data.twitter %>% 
    mutate(love = str_detect(words, "love"))%>%
    group_by(line) %>%
    summarise(love_by_line = sum(love)) %>% 
    filter(love_by_line != 0)

lines_hate <- data.twitter %>% 
    mutate(hate = str_detect(words, "hate"))%>%
    group_by(line) %>%
    summarise(hate_by_line = sum(hate)) %>% 
    filter(hate_by_line != 0)


dim(lines_love)[1] / dim(lines_hate)[1]
```
```{r biostats tweet}
data.raw.twitter[str_which(data.raw.twitter, " biostats ")]
```

```{r number tweets matching phrase}
str_which(data.raw.twitter, "A computer once beat me at chess, but it was no match for me at kickboxing") %>% 
    length()
```
#.
# Exploration
## Expectations in data:
- expecting words to be spelled correctly... most likely not true
- expecting tokenizer to correctly split words and drop punctuation... mostlikely not true

## Tasks to accomplish

- Exploratory analysis - perform a thorough exploratory analysis of the data, understanding the distribution of words and relationship between the words in the corpora.
- Understand frequencies of words and word pairs - build figures and tables to understand variation in the frequencies of words and word pairs in the data.

## Questions to consider
- Some words are more frequent than others - what are the distributions of word frequencies?
- What are the frequencies of 2-grams and 3-grams in the dataset?
- How many unique words do you need in a frequency sorted dictionary to cover 50% of all word instances in the language? 90%?
- How do you evaluate how many of the words come from foreign languages?
- Can you think of a way to increase the coverage -- identifying words that may not be in the corpora or using a smaller number of words in the dictionary to cover the same number of phrases?
```{r missing data?}
data.list <- list(data.blogs, data.news, data.twitter)

map(data.list, summary)
```
```{r proportion miss spelled words?}

data.token.1.list <- list(data.one.words.blogs, data.one.words.news, data.one.words.twitter)
sources <- c("blogs", "news", "twitter") 

map2_df(.x = data.token.1.list,
        .y = sources,
        .f = function(x, y){
            x %>% mutate(correct.spelling = hunspell_check(words)) %>%
                summarise(prop.spelling.errors = (dim(x)[1] - sum(correct.spelling))/dim(x)[1]) %>% 
                mutate(source = y)
            })
```
```{r proportion miss spelled words not english?}

data.token.1.list <- list(data.one.words.blogs, data.one.words.news, data.one.words.twitter)
sources <- c("blogs", "news", "twitter") 

map2_df(.x = data.token.1.list,
        .y = sources,
        .f = function(x, y){
            length.data <- dim(x)[1]
            
            miss.spell.en <- x %>% mutate(correct.spelling = hunspell_check(words),
                                          lang = detect_language(words)) %>%
                filter(correct.spelling == FALSE &
                           lang != "en") %>% 
                summarise(n = n(),
                          prop.miss.en = n/length.data) %>% 
                mutate(source = y)
            })
```
```{r what are the most common miss spelled words}
data.token.1.list <- list(data.one.words.blogs, data.one.words.news, data.one.words.twitter)
sources <- c("blogs", "news", "twitter") 

most_miss_spell <- function(x, y){
    x %>%
        mutate(correct.spelling = hunspell_check(words)) %>%
        filter(correct.spelling == FALSE) %>%
        count(words, sort = TRUE) %>% 
        mutate(source = y) %>% 
        head(n = 10)
    }

map2_df(.x = data.token.1.list,
        .y = sources,
        .f = most_miss_spell) %>%
    ggplot(aes(x = words, y = n, fill = source))+
    geom_col(position = position_dodge())+
    facet_wrap(~ source, nrow = 1)+
    coord_flip()

```
```{r what are the most common miss spelled english words}
data.token.1.list <- list(data.one.words.blogs, data.one.words.news, data.one.words.twitter)
sources <- c("blogs", "news", "twitter") 

most_miss_spell_english <- function(x, y){
    x %>%
        mutate(correct.spelling = hunspell_check(words),
               lang = detect_language(words)) %>%
        filter(correct.spelling == FALSE &
                   lang == "en") %>%
        count(words, sort = TRUE) %>% 
        mutate(source = y) %>% 
        head(n = 10)
    }

map2_df(.x = data.token.1.list,
        .y = sources,
        .f = most_miss_spell_english) %>%
    ggplot(aes(x = words, y = n, fill = source))+
    geom_col(position = position_dodge())+
    facet_wrap(~ source, nrow = 1)+
    coord_flip()

#test <- most_com_miss_spell(data.blogs, "blogs")
```
```{r dist of line lengths using purrr}
nwords <- function(x, y){
    x%>%
    group_by(line) %>% 
    summarise(nwords = n()) %>% 
    mutate(source = y)
}

data.list <- list(data.blogs, data.news, data.twitter)
sources <- c("blogs", "news", "twitter")

map2_df(.x = data.list, .y = sources, .f = nwords) %>% 
    ggplot(aes(x = nwords, fill = source))+
    facet_grid(source ~ .)+
    geom_histogram(bins = 100)+
    geom_rug()+
    scale_x_log10()
```

```{r distribution of word occurances}
data.list <- list(data.blogs, data.news, data.twitter)
sources <- c("blogs", "news", "twitter")

most_common_words <- function(x, y){
    x %>%
        count(words, sort = TRUE) %>%
        mutate(source = y)
    }
        
map2_df(.x = data.list, .y = sources, .f = most_common_words)%>% 
    ggplot(aes(x = n, fill = source))+
    facet_grid(source ~ .)+
    geom_histogram(bins = 100)
    

```
```{r distribution of word occurances log scale x and y}
data.list <- list(data.blogs, data.news, data.twitter)
sources <- c("blogs", "news", "twitter")

most_common_words <- function(x, y){
    x %>%
        count(words, sort = TRUE) %>%
        mutate(source = y)
    }
        
map2_df(.x = data.list, .y = sources, .f = most_common_words)%>% 
    ggplot(aes(x = n, fill = source))+
    facet_grid(source ~ .)+
    geom_histogram(bins = 100)+
    #geom_rug()+
    scale_x_log10()+
    scale_y_log10()

```

```{r bad words}
unique(data.blogs$words)[unique(data.blogs$words) %in% c("fuck", "shit", "pussy")]
```
```{r two word frequency dist}
top.first.words <- data.two.words.blogs %>% 
    separate(words, " ", into = c("first.word", "second.word")) %>% 
    count(first.word, sort = TRUE) %>% 
    head(n = 6)

data.two.words.blogs %>% 
    separate(words, " ", into = c("first.word", "second.word")) %>% 
    group_by(first.word) %>% 
    count(first.word, second.word, sort = TRUE) %>% 
    top_n(n, n = 5)
    

filter(first.word %in% top.first.words$first.word) %>% 
    ggplot(aes(x = second.word, fill = first.word))+
    geom_histogram()+
    facet_wrap(first.word~.)+
    scale_x_log10()
    

```

