---
title: "Next_word_pred_model"
author: "Joe Rubash"
date: "December 12, 2018"
output: html_document
---


# plan of attack ("make it work", "make it right", "make it fast")
## Stage one - make a simple model using calculated frequencies from n-grams
1. read in data and make a corpus with all sources
2. to start out sample a small portion of this corpus
3. use qunteda package to create unigram (to lower case, strip punc, remove extra 
white space. etc)
4. create a dictionary to assign integer to unique words in unigram
5. build unigram, bigram and trigram document term matrices
6. split ngrams to give previous word(s) and next word (to predict)
7. calc frequencies
8. to colsum of frequencies to
9. create backoff method to start with highest order n-gram then move to lowest
order n-gram if no solution found


## stage two use smoothing to improve prediction and various methods to spee up
8. figure out how to alter frequencies to predict unkowns
9. su

```{r setup - load data}
# clear environment
rm(list = ls())

knitr::opts_chunk$set(echo = TRUE)

# required libraries
library(quanteda)
library(data.table)
library(readtext)
library(readr)

# Download data-----------------------------------------------------------------
# download and unzip data folder
if(!dir.exists("../data/Coursera-SwiftKey/")){
    file.url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
    temp <- tempfile()
    download.file(file.url, temp, mode = "wb")
    unzip(temp, exdir = "../data/Coursera-SwiftKey")
    rm(temp)
}

# Sample data-------------------------------------------------------------------
# read in all english data files, sample then save samples to file
sample.size <- 0.05 # delete sample files and rerun code if changing sample size

# create function to read in convert, sample then write to file text data
cleanFiles <- function(file, newfile, sample.size){
    readLines(file,
              skipNul = TRUE,
              encoding="UTF-8") %>%
        iconv(.) %>%
        sample(.,
               size = sample.size * length(.),
               replace = FALSE) %>% 
    writeLines(.,newfile)
}

# sample blog data
if(!file.exists("../data/temp.data/sample.blogs.txt")){
    cleanFiles(file = "../data/Coursera-SwiftKey/final/en_US/en_US.blogs.txt",
               newfile = "../data/temp.data/sample.blogs.txt",
               sample.size = sample.size)
}
# sample news data
if(!file.exists("../data/temp.data/sample.news.txt")){
    cleanFiles(file = file("../data/Coursera-SwiftKey/final/en_US/en_US.news.txt", "rb"),
               newfile = "../data/temp.data/sample.news.txt",
               sample.size = sample.size)
}
# sample twitter data
if(!file.exists("../data/temp.data/sample.twitter.txt")){
    cleanFiles(file = "../data/Coursera-SwiftKey/final/en_US/en_US.twitter.txt",
               newfile = "../data/temp.data/sample.twitter.txt",
               sample.size = sample.size)
}

# Load sampled data-------------------------------------------------------------
# load blog data sample
data.blogs <- read.delim("../data/temp.data/sample.blogs.txt",
                                 sep = "\t",
                                 header = FALSE,
                                 stringsAsFactors = FALSE)
# load news data sample
data.news <- read.delim("../data/temp.data/sample.news.txt",
                            sep = "\t",
                            header = FALSE,
                            stringsAsFactors = FALSE)
# load twitter data sample
data.twitter <- read.delim("../data/temp.data/sample.twitter.txt",
                            sep = "\t",
                            header = FALSE,
                            stringsAsFactors = FALSE)

```


