---
title: "Next_word_pred_model"
author: "Joe Rubash"
date: "December 12, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# required libraries
library(quanteda)
library(data.table)
library(readtext)
library(readr)

```

# plan of attack ("make it work", "make it right", "make it fast")
## Stage one - make a simple model using calculated frequencies from n-grams
1. read in data and make a corpus with all sources
2. to start out sample a small portion of this corpus
3. use qunteda package to create unigram (to lower case, strip punc, remove extra 
white space. etc)
4. create a dictionary to assign integer to unique words in unigram
5. build unigram, bigram and trigram document term matrices
6. split ngrams to give previous word(s) and next word (to predict)
7. calc frequencies
8. to colsum of frequencies to
9. create backoff method to start with highest order n-gram then move to lowest
order n-gram if no solution found


## stage two use smoothing to improve prediction and various methods to spee up
8. figure out how to alter frequencies to predict unkowns
9. su

```{r}
# clear environment
rm(list = ls())

# Load data----
# download and unzip data folder
if(!dir.exists("../data/Coursera-SwiftKey/")){
    file.url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
    temp <- tempfile()
    download.file(file.url, temp, mode = "wb")
    unzip(temp, exdir = "../data/Coursera-SwiftKey")
    rm(temp)
}

# Load then sample a portion of data then save to file
sample.size <- 0.05 # proportion of orignal text data to save to file

# load en_US.blogs.txt----
if(file.exists("../data/temp.data/sample.blogs.txt")){
    sample.blogs <- read_tsv("../data/temp.data/sample.blogs.txt")
} else {
    data.raw.blogs <- readLines("../data/Coursera-SwiftKey/final/en_US/en_US.blogs.txt",
                                skipNul = TRUE,
                                encoding="UTF-8")
    # create a sample
    sample.blogs <- sample(data.raw.blogs,
                           size = sample.size * length(data.raw.blogs),
                           replace = FALSE)
    # write the sample to txt file in "output"
    writeLines(sample.blogs,
               "../data/temp.data/sample.blogs.txt")
}

# load en_US.news.txt----
if(file.exists("../data/temp.data/sample.news.txt")){
    sample.news <- read_tsv("../data/temp.data/sample.news.txt")
} else {
    data.raw.news <- readLines(con = file("../data/Coursera-SwiftKey/final/en_US/en_US.news.txt", "rb"),
                               skipNul = TRUE)
    # create a sample
    sample.news <- sample(data.raw.news,
                          size = sample.size * length(data.raw.news),
                          replace = FALSE)
    # write the sample to txt file in "output"
    writeLines(sample.news,
               "../data/temp.data/sample.news.txt")
}

# load en_US.twitter.txt----
if(file.exists("../data/temp.data/sample.twitter.txt")){
    sample.twitter <- read_tsv("../data/temp.data/sample.twitter.txt")
} else {
    data.raw.twitter <- readLines("../data/Coursera-SwiftKey/final/en_US/en_US.twitter.txt",
                                  skipNul = TRUE,
                                  encoding="UTF-8")
    # create a sample
    sample.twitter <- sample(data.raw.twitter,
                             size = sample.size * length(data.raw.twitter),
                             replace = FALSE)
    # write the sample to txt file in "output"
    writeLines(sample.twitter,
               "../data/temp.data/sample.twitter.txt")
}

# Add line variable to data----
data.blogs <- data.table(line = 1:length(sample.blogs),
                     txt_sample = sample.blogs)
data.news <- data.table(line = 1:length(sample.news),
                    txt_sample = sample.news)
data.twitter <- data.table(line = 1:length(sample.twitter),
                       txt_sample = sample.twitter)

```
```{r Test to eval read text file times, eval=FALSE, include=FALSE}
# readr package
# start time
pt <- proc.time() 
readr.sample.twitter <- read_tsv(file = "../data/temp.data/sample.twitter.txt")
#end time
proc.time() - pt

# start time
pt <- proc.time() 
fread.sample.twitter <- fread(input = "../data/temp.data/sample.twitter.txt")
#end time
proc.time() - pt
```
```{r Test to eval read text file times, eval=FALSE, include=FALSE}
# readr package
# start time
pt <- proc.time() 
readtext.twitter <- read.table(file = "../data/temp.data/sample.twitter.txt",
                             encoding = "UTF-8",
                             header = FALSE)
#end time
proc.time() - pt

# start time
pt <- proc.time() 
readLines.twitter <- readLines("../data/Coursera-SwiftKey/final/en_US/en_US.twitter.txt",
                                  skipNul = TRUE,
                                  encoding="UTF-8")
#end time
proc.time() - pt
```
```{r other methods to read in data}
# create function to read in convert, sample then write to file text data
cleanFiles <- function(file, newfile, sample.size){
    readLines(file,
              skipNul = TRUE,
              encoding="UTF-8") %>%
        iconv(.) %>%
        sample(.,
               size = sample.size * length(.),
               replace = FALSE) %>% 
    writeLines(.,newfile)
}

cleanFiles(file = "../data/Coursera-SwiftKey/final/en_US/en_US.twitter.txt",
           newfile = "../data/temp.data/data.twitter.txt",
           sample.size = 0.05)

# start time
pt <- proc.time()
data.twitter.fread <- fread("../data/temp.data/data.twitter.txt",
                            sep = "\t",
                            header = FALSE)
#end time
proc.time() - pt
```

